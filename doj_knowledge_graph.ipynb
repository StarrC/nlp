{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447f3198",
   "metadata": {},
   "source": [
    "Reference: https://www.kaggle.com/code/pavansanagapati/knowledge-graph-nlp-tutorial-bert-spacy-nltk/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14e003c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import requests\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1483831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48050, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import DOJ sentences\n",
    "candidate_sentences = pd.read_csv(\"doj_api_data_new.csv\")\n",
    "candidate_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2873294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8280     BUFFALO, N.Y.--U.S. Attorney William J. Hochul, Jr. announced today that Myron Johnson, 39, of Buffalo, N.Y., who was convicted of possession with intent to distribute cocaine, was sentenced to 51...\n",
       "24831    Follow @SDILNews \\n\\nNAEEM MAHMOOD KOHLI, 60, of Effingham, Illinois, was convicted of seven counts of illegal dispensation of a Schedule II Controlled Substance following a 17-day jury trial held...\n",
       "4303     WASHINGTON – The Department of Justice and the Department of the Interior announced today that Freeport-McMoRan Corporation and Freeport-McMoRan Morenci Inc. (Freeport-McMoRan) have agreed to pay ...\n",
       "4782     A federal judge in Worcester, Mass., sentenced William Scott Dion today to 84 months in prison for conspiring to defraud the United States, and for obstructing the Internal Revenue Service (IRS), ...\n",
       "33373    St. Louis, MO – JOEY D. WOOD pled  guilty to filing four false tax returns for himself and two others claiming  refunds totaling over $23,000 for tax years 2011 and 2012. According to court docume...\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_sentences['body'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba00f10",
   "metadata": {},
   "source": [
    "### Entities Extraction\n",
    "To build a knowledge graph, the most important things are the nodes and the edges between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f78d8bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "  ## chunk 1\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "\n",
    "  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "  prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "  prefix = \"\"\n",
    "  modifier = \"\"\n",
    "\n",
    "  #############################################################\n",
    "  \n",
    "  for tok in nlp(sent):\n",
    "    ## chunk 2\n",
    "    # if token is a punctuation mark then move on to the next token\n",
    "    if tok.dep_ != \"punct\":\n",
    "      # check: token is a compound word or not\n",
    "      if tok.dep_ == \"compound\":\n",
    "        prefix = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          prefix = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      # check: token is a modifier or not\n",
    "      if tok.dep_.endswith(\"mod\") == True:\n",
    "        modifier = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          modifier = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      ## chunk 3\n",
    "      if tok.dep_.find(\"subj\") == True:\n",
    "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"      \n",
    "\n",
    "      ## chunk 4\n",
    "      if tok.dep_.find(\"obj\") == True:\n",
    "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        \n",
    "      ## chunk 5  \n",
    "      # update variables\n",
    "      prv_tok_dep = tok.dep_\n",
    "      prv_tok_text = tok.text\n",
    "  #############################################################\n",
    "\n",
    "  return [ent1.strip(), ent2.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ccc01d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['film', '200  patents']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_entities(\"the film had 200 patents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "739b43ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 48050/48050 [2:02:33<00:00,  6.53it/s]\n"
     ]
    }
   ],
   "source": [
    "entity_pairs = []\n",
    "\n",
    "for i in tqdm(candidate_sentences[\"body\"]):\n",
    "  entity_pairs.append(get_entities(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3e052de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dangerous Ervin Prenci', 'criminal Fugitive Investigative More'],\n",
       " ['30 Americas participants', 'legal American States'],\n",
       " ['serious  ERO', 'illegally United country'],\n",
       " ['', ''],\n",
       " ['I', 'forward  questions'],\n",
       " ['Western office', 'crucial  David'],\n",
       " ['so cyber we', 'important  me'],\n",
       " ['also  I', 'forward  questions'],\n",
       " ['international  efforts', 'currently Northern visit'],\n",
       " ['true Justice Department', 'passing']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_pairs[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6074f2c1",
   "metadata": {},
   "source": [
    "### Relationship Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ffe33c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation(sent):\n",
    "\n",
    "  doc = nlp(sent)\n",
    "\n",
    "  # Matcher class object \n",
    "  matcher = Matcher(nlp.vocab)\n",
    "\n",
    "  #define the pattern \n",
    "  pattern = [{'DEP':'ROOT'}, \n",
    "            {'DEP':'prep','OP':\"?\"},\n",
    "            {'DEP':'agent','OP':\"?\"},  \n",
    "            {'POS':'ADJ','OP':\"?\"}] \n",
    "\n",
    "  matcher.add(\"matching_1\",[pattern]) \n",
    "\n",
    "  matches = matcher(doc)\n",
    "  k = len(matches) - 1\n",
    "\n",
    "  span = doc[matches[k][1]:matches[k][2]] \n",
    "\n",
    "  return(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cea9e59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'completed'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_relation(\"John completed the task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfdbd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████▎                              | 7059/48050 [10:58<1:11:41,  9.53it/s]"
     ]
    }
   ],
   "source": [
    "relations = [get_relation(i) for i in tqdm(candidate_sentences['body'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d21df7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'relations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pd\u001b[38;5;241m.\u001b[39mSeries(\u001b[43mrelations\u001b[49m)\u001b[38;5;241m.\u001b[39mvalue_counts()[:\u001b[38;5;241m50\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'relations' is not defined"
     ]
    }
   ],
   "source": [
    "pd.Series(relations).value_counts()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefb74b5",
   "metadata": {},
   "source": [
    "Time it took to do it. Whenever doing with GPT model will need to develop prompts but doing through the API have to write code. If want relationships in a specif format, would have to create relationships into a file. But then how does Spacy do that? Is that the same order of time to do that in a GPT model? Can it do it automatically and faster? Is there a much lower bar to make it happen? Even if Spacy is better. \n",
    "\n",
    "When we start evaluating downfalls of these models that they don't have memory. The Raven one has infinite memory but looking at GPT based ones we have to use zero shot memory by injecting into data prompts themselves. Are there things that we can do to improve results? Is there an idea around extracting relationship information that even if GPT model does it so now we can inject it inot zero shot lerning to improve how it eva,uates on subsequent topics. Zero shot training. AutoGPT and Jarvis from microsoft is the corrollary to that. \n",
    "\n",
    "RE from Spacy and GPT comparison\n",
    "Article is trying to visualize LLM as it's being trained. Look at this capability. Knowledge of the embedding space. \n",
    "Taking HT data, parsing into sections that make sense. Whether that's paragraphs and creating summaries of those sections and using one of the embedding systems to store them in a vector set to use for searching. Now can take prompts from LLM and pass it's concepts and get similaritiues from vector data space and inject those summaries intot he prompt so giving LLM. Doing preprocessing step by taking initial query and before pass it to the LLM getting it's embeddings from that data vector base are relevant and getting articles linked into embeddings. Now prompt will have original question and utilizing info from vector and all of that goes into the prompt. If want the bot to answer a certain way, can use data provided to answer. Will have command of the language from LLM but have relevant information from data set. Have to summarize prompts to have that new vector background info. \n",
    "\n",
    "In a HT use case, I want o figure out what places and why they are utilized for trafficking locations? The LLM says: what would I need to do to figure that out: break down into tasks and decides to do web queries, do stat analysis etc. And tasks are autogenerated and at the end it provides a result based on all the tasks it's created. \n",
    "\n",
    "Worthwhile getting exposed to all of this. Use the latest and greatest model. LLMs 3.5 and 4. Getting a feel for capacity and capability. \n",
    "\n",
    "Looking at trying to utilize this and figure out weakensses and strengths and what can be done to improve those weaknesses. Touch as many tools and things: pine cone for vector databases, autoGPT for task creation. \n",
    "\n",
    "Creating pinecone account to create vector database for long-term memory. \n",
    "\n",
    "Preprocessing prompt to look for similarities. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
