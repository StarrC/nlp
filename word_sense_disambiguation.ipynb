{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSceg+GC8o0JM/8SRaIZEY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StarrC/nlp/blob/main/word_sense_disambiguation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Sense Disambiguation Report** \n",
        "### By Starr Corbin"
      ],
      "metadata": {
        "id": "yutuHL_QOfXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this project, Machine learning was incorporated using training data to improve the accuracy of the word sense disambiguation (WSD) process. The goal being to potentially improve the accuracy of the WSD process by using patterns in the training data to better classify the senses of the target word. Here is an overview of the approach used: \n",
        "\n",
        "1. Use a test corpus of text that includes sentences with the target words of \"yarn\", \"rubbish\", and \"tissue\" and their corresponding senses.\n",
        "3. Preprocess the text by doing lemmatization, removing stop words, punctuation, blank spaces and lower casing the words in both the training and test sets.\n",
        "4. Create a function for word sensing using the Lesk algorithm and perform word sense disambiguation labeling on the test corpus. \n",
        "5. Convert the text to numerical features using TF-IDF vectorization that can be used as input to a machine learning model. \n",
        "5. Split the corpus into a training set and a test set.\n",
        "6. Train a logistic regression model on the training set to classify the senses of for each target word (yarn, rubbish and tissue).\n",
        "7. Test the model on the test set and print accuracy\n",
        "8. Evaluate the performance of the model on the test set\n",
        "by using the trained model to predict the senses of the word \"yarn\" in new, unseen text."
      ],
      "metadata": {
        "id": "jq0Ahw7SeyKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, The code performs natural language processing (NLP) tasks on a text file named \"yarn.txt\". The file is preprocessed and cleaned using various NLP techniques and a logistic regression model is trained on the preprocessed data to predict labels for the text file.\n",
        "\n",
        "The code starts by importing the necessary libraries from the Natural Language Toolkit (nltk) such as stopwords, word_tokenize, WordNetLemmatizer, and string. It then defines a function \"preprocess\" that takes a sentence as an input and preprocesses it by removing punctuation, tokenizing the sentence into words, removing stop words, lemmatizing the words, and joining the words back into a sentence. This function is later used to preprocess the \"yarn.txt\" file.\n",
        "\n",
        "Next, the code reads the \"yarn.txt\" file and saves each sentence in a list after stripping the new line characters. It then preprocesses each sentence in the list using the \"preprocess\" function and saves the cleaned sentences in a new file named \"cleaned_yarn.txt\".\n",
        "\n",
        "After that, a function \"WSD_Test_Yarn\" is defined that takes a list of sentences as an input and performs Word Sense Disambiguation (WSD) on each sentence using Lesk algorithm to identify the sense of the word \"yarn\" in the sentence. The function returns a list of senses for each sentence where 1 represents the first sense of the word \"yarn\" and 2 represents the second sense.\n",
        "\n",
        "The code then reads the preprocessed sentences from the \"cleaned_yarn.txt\" file and performs WSD on each sentence using the \"WSD_Test_Yarn\" function. It assigns a label to each sentence based on the sense of the word \"yarn\" in the sentence. If the sense is 1, the label is \"sense 1\", and if the sense is 2, the label is \"sense 2\". If the word \"yarn\" is not found in a sentence, the label is \"skip\". The labels are saved in a new file named \"yarn_labels.txt\".\n",
        "\n",
        "Finally, the code uses the preprocessed sentences from the \"cleaned_yarn.txt\" file and the labels from the \"yarn_labels.txt\" file to train a logistic regression model using the TF-IDF vectorization technique. The preprocessed text is converted into numerical features using TF-IDF vectorization, and the data is split into training and test sets. The logistic regression model is trained on the training set and tested on the test set. The accuracy of the model is printed to the console."
      ],
      "metadata": {
        "id": "M5f_8cRobii9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Define the function to clean the text\n",
        "def preprocess(sentence):\n",
        "    # Remove blank lines\n",
        "    if not sentence.strip():\n",
        "        return \"\"\n",
        "\n",
        "    # Remove punctuation\n",
        "    sentence = sentence.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    # Remove numbers\n",
        "    sentence = re.sub(r'\\d+', '', sentence)\n",
        "    # Tokenize the sentence\n",
        "    words = word_tokenize(sentence.lower())\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    words = [word for word in words if not word in stop_words]\n",
        "    # Lemmatize the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    # Join the words back into a sentence\n",
        "    cleaned_sentence = \" \".join(words)\n",
        "    return cleaned_sentence\n",
        "\n",
        "# Open the yarn input file and read the sentences\n",
        "with open(\"yarn.txt\", \"r\") as f:\n",
        "    sentences = [line.strip() for line in f]\n",
        "\n",
        "# Clean the yarn sentences and save to a new file\n",
        "with open(\"cleaned_yarn.txt\", \"w\") as f:\n",
        "    for sentence in sentences:\n",
        "        cleaned_sentence = preprocess(sentence)\n",
        "        f.write(cleaned_sentence + \"\\n\")\n",
        "\n",
        "def WSD_Test_Yarn(list):\n",
        "    senses = []\n",
        "    for sentence in list:\n",
        "        # Preprocess sentence\n",
        "        words = preprocess(sentence)\n",
        "        # Get the word \"yarn\" and its context\n",
        "        target_word = \"yarn\"\n",
        "        target_index = words.index(target_word)\n",
        "        context = set(words[:target_index] + words[target_index+1:])\n",
        "        # Perform Lesk algorithm for word sense disambiguation\n",
        "        synsets = wn.synsets(target_word)\n",
        "        best_sense = None\n",
        "        max_overlap = 0\n",
        "        for i, synset in enumerate(synsets):\n",
        "            definition = set(preprocess(synset.definition()))\n",
        "            examples = set(preprocess(\" \".join(synset.examples())))\n",
        "            signature = definition.union(examples)\n",
        "            overlap = len(context.intersection(signature))\n",
        "            if overlap > max_overlap:\n",
        "                max_overlap = overlap\n",
        "                best_sense = i\n",
        "        if best_sense == 0:\n",
        "            senses.append(1)\n",
        "        else:\n",
        "            senses.append(2)\n",
        "    return senses\n",
        "\n",
        "# Load preprocessed sentences\n",
        "with open(\"cleaned_yarn.txt\", \"r\") as f:\n",
        "    sentences = [line.strip() for line in f]\n",
        "\n",
        "# Perform WSD on each sentence and assign label\n",
        "labels = []\n",
        "for sentence in sentences:\n",
        "    if \"yarn\" in sentence:\n",
        "        sense = WSD_Test_Yarn([sentence])[0]\n",
        "        if sense == 1:\n",
        "            labels.append(\"sense 1\")\n",
        "        else:\n",
        "            labels.append(\"sense 2\")\n",
        "    else:\n",
        "        # Skip sentence if target word not found\n",
        "        labels.append(\"skip\")\n",
        "\n",
        "# Write labels to file\n",
        "with open(\"yarn_labels.txt\", \"w\") as f:\n",
        "    for label in labels:\n",
        "        f.write(label + \"\\n\")\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Read preprocessed text\n",
        "with open(\"cleaned_yarn.txt\", \"r\") as f:\n",
        "    sentences = f.readlines()\n",
        "\n",
        "# Read labels\n",
        "with open(\"cleaned_yarn.txt\", \"r\") as f:\n",
        "    sentences = f.readlines()\n",
        "with open(\"yarn_labels.txt\", \"r\") as f:\n",
        "    labels_dict = {\"sense 1\": 1, \"sense 2\": 2}\n",
        "    labels = [labels_dict[line.strip()] if line.strip() != 'skip' else None for line in f]\n",
        "\n",
        "# Convert text to numerical features using TF-IDF vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform([sentences[i] for i in range(len(sentences)) if labels[i] is not None])\n",
        "y = [label for label in labels if label is not None]\n",
        "\n",
        "# Split data into training and test sets\n",
        "train_size = int(X.shape[0] * 0.8)\n",
        "X_train = X[:train_size]\n",
        "X_test = X[train_size:]\n",
        "y_train = y[:train_size]\n",
        "y_test = y[train_size:]\n",
        "\n",
        "# Train a logistic regression model on the training set\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Test the model on the test set and print accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0CgmgLO9v2j",
        "outputId": "e8e81090-1074-49d4-816a-f8d131877818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next block of code is for testing the trained classifier on new text data. \n",
        "\n",
        "First, it imports the necessary libraries, which are pandas and TfidfVectorizer from sklearn's feature extraction module.\n",
        "\n",
        "Next, it reads in a new text file called \"yarn_testdata.txt\" and preprocesses each sentence in the file using the previously defined \"preprocess\" function.\n",
        "\n",
        "Then, it uses the TfidfVectorizer object to convert the preprocessed text into numerical features. This is done using the \"transform\" method of the vectorizer object.\n",
        "\n",
        "After that, it uses the previously trained logistic regression classifier to predict the senses for the new text. This is done using the \"predict\" method of the classifier object on the new features.\n",
        "\n",
        "Finally, it prints the predicted senses for each sentence in the new text. If the sentence contains the word \"yarn\", it prints the predicted sense for that sentence. Otherwise, it prints \"skip\" indicating that the sentence was skipped because it does not contain the target word. The predicted senses are printed in the format \"Sentence [number]: [sense]\". The number indicates the index of the sentence in the input file, and the sense is either \"sense 1\" or \"sense 2\"."
      ],
      "metadata": {
        "id": "NngJWH8YDk6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Read in the new text file\n",
        "with open(\"yarn_testdata.txt\", \"r\") as f:\n",
        "    sentences = [line.strip() for line in f]\n",
        "\n",
        "# Clean the new test sentences and save to a new file\n",
        "with open(\"cleaned_yarn_testdata.txt\", \"w\") as f:\n",
        "    for sentence in sentences:\n",
        "        cleaned_sentence = preprocess(sentence)\n",
        "        f.write(cleaned_sentence + \"\\n\")\n",
        "\n",
        "# Use the vectorizer object to convert the preprocessed text into numerical features\n",
        "X_new = vectorizer.transform(cleaned_sentences)\n",
        "\n",
        "# Use the trained classifier to make predictions on the new features\n",
        "y_pred = clf.predict(X_new)\n",
        "\n",
        "# Print the predicted senses for the new text\n",
        "for i, sentence in enumerate(sentences):\n",
        "    if \"yarn\" in sentence:\n",
        "        print(f\"Sentence {i+1}: {y_pred[i]}\")\n",
        "    else:\n",
        "        print(f\"Sentence {i+1}: skip\")\n",
        "\n",
        "# Export the predicted senses for the new text to a new file\n",
        "with open(\"result_yarn_starrcorbin.txt\", \"w\") as f:\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        if \"yarn\" in sentence:\n",
        "            f.write(f\"Sentence {i+1}: {y_pred[i]}\\n\")\n",
        "        else:\n",
        "            f.write(f\"Sentence {i+1}: skip\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMXJgUZcCW8j",
        "outputId": "9714359e-0004-457f-942d-50849ce07ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: 2\n",
            "Sentence 2: 2\n",
            "Sentence 3: 2\n",
            "Sentence 4: 2\n",
            "Sentence 5: 2\n",
            "Sentence 6: 2\n",
            "Sentence 7: 2\n",
            "Sentence 8: 2\n",
            "Sentence 9: 2\n",
            "Sentence 10: 2\n",
            "Sentence 11: 2\n",
            "Sentence 12: 2\n",
            "Sentence 13: 2\n",
            "Sentence 14: 2\n",
            "Sentence 15: 2\n",
            "Sentence 16: 2\n",
            "Sentence 17: 2\n",
            "Sentence 18: 2\n",
            "Sentence 19: 2\n",
            "Sentence 20: 2\n",
            "Sentence 21: 2\n",
            "Sentence 22: 2\n",
            "Sentence 23: 2\n",
            "Sentence 24: 2\n",
            "Sentence 25: 2\n",
            "Sentence 26: 2\n",
            "Sentence 27: 2\n",
            "Sentence 28: 2\n",
            "Sentence 29: 2\n",
            "Sentence 30: 2\n",
            "Sentence 31: 2\n",
            "Sentence 32: 2\n",
            "Sentence 33: 2\n",
            "Sentence 34: 2\n",
            "Sentence 35: 2\n",
            "Sentence 36: 2\n",
            "Sentence 37: 2\n",
            "Sentence 38: 2\n",
            "Sentence 39: 2\n",
            "Sentence 40: 2\n",
            "Sentence 41: 2\n",
            "Sentence 42: 2\n",
            "Sentence 43: 2\n",
            "Sentence 44: 2\n",
            "Sentence 45: 2\n",
            "Sentence 46: 2\n",
            "Sentence 47: 2\n",
            "Sentence 48: 2\n",
            "Sentence 49: 2\n",
            "Sentence 50: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform the same machine learning and word senseing process for rubbish and then tissue. "
      ],
      "metadata": {
        "id": "as1Nw0gTGXG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the rubbish input file and read the sentences\n",
        "with open(\"rubbish.txt\", \"r\") as f:\n",
        "    sentences = [line.strip() for line in f]\n",
        "\n",
        "# Clean the rubbish sentences and save to a new file\n",
        "with open(\"cleaned_rubbish.txt\", \"w\") as f:\n",
        "    for sentence in sentences:\n",
        "        cleaned_sentence = preprocess(sentence)\n",
        "        f.write(cleaned_sentence + \"\\n\")\n",
        "\n",
        "def WSD_Test_Rubbish(list):\n",
        "    senses = []\n",
        "    for sentence in list:\n",
        "        # Preprocess sentence\n",
        "        words = preprocess(sentence)\n",
        "        # Get the word \"rubbish\" and its context\n",
        "        target_word = \"rubbish\"\n",
        "        target_index = words.index(target_word)\n",
        "        context = set(words[:target_index] + words[target_index+1:])\n",
        "        # Perform Lesk algorithm for word sense disambiguation\n",
        "        synsets = wn.synsets(target_word)\n",
        "        best_sense = None\n",
        "        max_overlap = 0\n",
        "        for i, synset in enumerate(synsets):\n",
        "            definition = set(preprocess(synset.definition()))\n",
        "            examples = set(preprocess(\" \".join(synset.examples())))\n",
        "            signature = definition.union(examples)\n",
        "            overlap = len(context.intersection(signature))\n",
        "            if overlap > max_overlap:\n",
        "                max_overlap = overlap\n",
        "                best_sense = i\n",
        "        if best_sense == 0:\n",
        "            senses.append(1)\n",
        "        else:\n",
        "            senses.append(2)\n",
        "    return senses\n",
        "\n",
        "# Load preprocessed sentences\n",
        "with open(\"cleaned_rubbish.txt\", \"r\") as f:\n",
        "    sentences = [line.strip() for line in f]\n",
        "\n",
        "# Perform WSD on each sentence and assign label\n",
        "labels = []\n",
        "for sentence in sentences:\n",
        "    if \"rubbish\" in sentence:\n",
        "        sense = WSD_Test_Rubbish([sentence])[0]\n",
        "        if sense == 1:\n",
        "            labels.append(\"sense 1\")\n",
        "        else:\n",
        "            labels.append(\"sense 2\")\n",
        "    else:\n",
        "        # Skip sentence if target word not found\n",
        "        labels.append(\"skip\")\n",
        "\n",
        "# Write labels to file\n",
        "with open(\"rubbish_labels.txt\", \"w\") as f:\n",
        "    for label in labels:\n",
        "        f.write(label + \"\\n\")\n",
        "\n",
        "# Read preprocessed text\n",
        "with open(\"cleaned_rubbish.txt\", \"r\") as f:\n",
        "    sentences = f.readlines()\n",
        "\n",
        "# Read labels\n",
        "with open(\"cleaned_rubbish.txt\", \"r\") as f:\n",
        "    sentences = f.readlines()\n",
        "with open(\"rubbish_labels.txt\", \"r\") as f:\n",
        "    labels_dict = {\"sense 1\": 1, \"sense 2\": 2}\n",
        "    labels = [labels_dict[line.strip()] if line.strip() != 'skip' else None for line in f]\n",
        "\n",
        "# Convert text to numerical features using TF-IDF vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform([sentences[i] for i in range(len(sentences)) if labels[i] is not None])\n",
        "y = [label for label in labels if label is not None]\n",
        "\n",
        "# Split data into training and test sets\n",
        "train_size = int(X.shape[0] * 0.8)\n",
        "X_train = X[:train_size]\n",
        "X_test = X[train_size:]\n",
        "y_train = y[:train_size]\n",
        "y_test = y[train_size:]\n",
        "\n",
        "# Train a logistic regression model on the training set\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Test the model on the test set and print accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17f9b276-ea59-40a2-eab3-80ce3019f7a9",
        "id": "-RqjxnIWKxWN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9090909090909091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the new text file\n",
        "with open(\"rubbish_testdata.txt\", \"r\") as f:\n",
        "    sentences = [line.strip() for line in f]\n",
        "\n",
        "# Clean the new test sentences and save to a new file\n",
        "with open(\"cleaned_rubbish_testdata.txt\", \"w\") as f:\n",
        "    for sentence in sentences:\n",
        "        cleaned_sentence = preprocess(sentence)\n",
        "        f.write(cleaned_sentence + \"\\n\")\n",
        "\n",
        "# Use the vectorizer object to convert the preprocessed text into numerical features\n",
        "X_new = vectorizer.transform(cleaned_sentences)\n",
        "\n",
        "# Use the trained classifier to make predictions on the new features\n",
        "y_pred = clf.predict(X_new)\n",
        "\n",
        "# Print the predicted senses for the new text\n",
        "for i, sentence in enumerate(sentences):\n",
        "    if \"rubbish\" in sentence:\n",
        "        print(f\"Sentence {i+1}: {y_pred[i]}\")\n",
        "    else:\n",
        "        print(f\"Sentence {i+1}: skip\")\n",
        "\n",
        "# Export the predicted senses for the new text to a new file\n",
        "with open(\"result_rubbish_starrcorbin.txt\", \"w\") as f:\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        if \"rubbish\" in sentence:\n",
        "            f.write(f\"Sentence {i+1}: {y_pred[i]}\\n\")\n",
        "        else:\n",
        "            f.write(f\"Sentence {i+1}: skip\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd46ab8d-44d3-4daf-e86c-60ac17650f41",
        "id": "rKrtxuTYG6uz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: 1\n",
            "Sentence 2: 1\n",
            "Sentence 3: 1\n",
            "Sentence 4: 1\n",
            "Sentence 5: 1\n",
            "Sentence 6: 1\n",
            "Sentence 7: 1\n",
            "Sentence 8: 1\n",
            "Sentence 9: 1\n",
            "Sentence 10: 1\n",
            "Sentence 11: 1\n",
            "Sentence 12: 1\n",
            "Sentence 13: 1\n",
            "Sentence 14: 1\n",
            "Sentence 15: 1\n",
            "Sentence 16: 1\n",
            "Sentence 17: 1\n",
            "Sentence 18: 1\n",
            "Sentence 19: 1\n",
            "Sentence 20: 1\n",
            "Sentence 21: 1\n",
            "Sentence 22: 1\n",
            "Sentence 23: 1\n",
            "Sentence 24: 1\n",
            "Sentence 25: 1\n",
            "Sentence 26: 1\n",
            "Sentence 27: 1\n",
            "Sentence 28: 1\n",
            "Sentence 29: 1\n",
            "Sentence 30: 1\n",
            "Sentence 31: 1\n",
            "Sentence 32: 1\n",
            "Sentence 33: 1\n",
            "Sentence 34: 1\n",
            "Sentence 35: 1\n",
            "Sentence 36: 1\n",
            "Sentence 37: 1\n",
            "Sentence 38: 1\n",
            "Sentence 39: 1\n",
            "Sentence 40: 1\n",
            "Sentence 41: 1\n",
            "Sentence 42: 1\n",
            "Sentence 43: 1\n",
            "Sentence 44: 1\n",
            "Sentence 45: 1\n",
            "Sentence 46: 1\n",
            "Sentence 47: 1\n",
            "Sentence 48: 1\n",
            "Sentence 49: 1\n",
            "Sentence 50: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "84xglF_JL14X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the rissue input file and read the sentences\n",
        "with open(\"tissue.txt\", \"r\") as f:\n",
        "    sentences = [line.strip() for line in f]\n",
        "\n",
        "# Clean the tissue sentences and save to a new file\n",
        "with open(\"cleaned_tissue.txt\", \"w\") as f:\n",
        "    for sentence in sentences:\n",
        "        cleaned_sentence = preprocess(sentence)\n",
        "        f.write(cleaned_sentence + \"\\n\")\n",
        "\n",
        "def WSD_Test_Tissue(list):\n",
        "    senses = []\n",
        "    for sentence in list:\n",
        "        # Preprocess sentence\n",
        "        words = preprocess(sentence)\n",
        "        # Get the word \"tissue\" and its context\n",
        "        target_word = \"tissue\"\n",
        "        target_index = words.index(target_word)\n",
        "        context = set(words[:target_index] + words[target_index+1:])\n",
        "        # Perform Lesk algorithm for word sense disambiguation\n",
        "        synsets = wn.synsets(target_word)\n",
        "        best_sense = None\n",
        "        max_overlap = 0\n",
        "        for i, synset in enumerate(synsets):\n",
        "            definition = set(preprocess(synset.definition()))\n",
        "            examples = set(preprocess(\" \".join(synset.examples())))\n",
        "            signature = definition.union(examples)\n",
        "            overlap = len(context.intersection(signature))\n",
        "            if overlap > max_overlap:\n",
        "                max_overlap = overlap\n",
        "                best_sense = i\n",
        "        if best_sense == 0:\n",
        "            senses.append(1)\n",
        "        else:\n",
        "            senses.append(2)\n",
        "    return senses\n",
        "\n",
        "# Load preprocessed sentences\n",
        "with open(\"cleaned_tissue.txt\", \"r\") as f:\n",
        "    sentences = [line.strip() for line in f]\n",
        "\n",
        "# Perform WSD on each sentence and assign label\n",
        "labels = []\n",
        "for sentence in sentences:\n",
        "    if \"tissue\" in sentence:\n",
        "        sense = WSD_Test_Tissue([sentence])[0]\n",
        "        if sense == 1:\n",
        "            labels.append(\"sense 1\")\n",
        "        else:\n",
        "            labels.append(\"sense 2\")\n",
        "    else:\n",
        "        # Skip sentence if target word not found\n",
        "        labels.append(\"skip\")\n",
        "\n",
        "# Write labels to file\n",
        "with open(\"tissue_labels.txt\", \"w\") as f:\n",
        "    for label in labels:\n",
        "        f.write(label + \"\\n\")\n",
        "\n",
        "# Read preprocessed text\n",
        "with open(\"cleaned_tissue.txt\", \"r\") as f:\n",
        "    sentences = f.readlines()\n",
        "\n",
        "# Read labels\n",
        "with open(\"cleaned_tissue.txt\", \"r\") as f:\n",
        "    sentences = f.readlines()\n",
        "with open(\"tissue_labels.txt\", \"r\") as f:\n",
        "    labels_dict = {\"sense 1\": 1, \"sense 2\": 2}\n",
        "    labels = [labels_dict[line.strip()] if line.strip() != 'skip' else None for line in f]\n",
        "\n",
        "# Convert text to numerical features using TF-IDF vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform([sentences[i] for i in range(len(sentences)) if labels[i] is not None])\n",
        "y = [label for label in labels if label is not None]\n",
        "\n",
        "# Split data into training and test sets\n",
        "train_size = int(X.shape[0] * 0.8)\n",
        "X_train = X[:train_size]\n",
        "X_test = X[train_size:]\n",
        "y_train = y[:train_size]\n",
        "y_test = y[train_size:]\n",
        "\n",
        "# Train a logistic regression model on the training set\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Test the model on the test set and print accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08d79ad0-1288-4301-d821-1719a3c96afc",
        "id": "3kSIfTFxL2QI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the new text file\n",
        "with open(\"tissue_testdata.txt\", \"r\") as f:\n",
        "    sentences = [line.strip() for line in f]\n",
        "\n",
        "# Clean the new test sentences and save to a new file\n",
        "with open(\"cleaned_tissue_testdata.txt\", \"w\") as f:\n",
        "    for sentence in sentences:\n",
        "        cleaned_sentence = preprocess(sentence)\n",
        "        f.write(cleaned_sentence + \"\\n\")\n",
        "\n",
        "# Use the vectorizer object to convert the preprocessed text into numerical features\n",
        "X_new = vectorizer.transform(cleaned_sentences)\n",
        "\n",
        "# Use the trained classifier to make predictions on the new features\n",
        "y_pred = clf.predict(X_new)\n",
        "\n",
        "# Print the predicted senses for the new text\n",
        "for i, sentence in enumerate(sentences):\n",
        "    if \"tissue\" in sentence:\n",
        "        print(f\"Sentence {i+1}: {y_pred[i]}\")\n",
        "    else:\n",
        "        print(f\"Sentence {i+1}: skip\")\n",
        "\n",
        "# Export the predicted senses for the new text to a new file\n",
        "with open(\"result_tissue_starrcorbin.txt\", \"w\") as f:\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        if \"tissue\" in sentence:\n",
        "            f.write(f\"Sentence {i+1}: {y_pred[i]}\\n\")\n",
        "        else:\n",
        "            f.write(f\"Sentence {i+1}: skip\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d716bad3-0ddf-4996-d8d4-a8cb305e3ddb",
        "id": "ToBYdCH6MQUi"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: 2\n",
            "Sentence 2: 2\n",
            "Sentence 3: 2\n",
            "Sentence 4: 2\n",
            "Sentence 5: 2\n",
            "Sentence 6: 2\n",
            "Sentence 7: 2\n",
            "Sentence 8: 2\n",
            "Sentence 9: 2\n",
            "Sentence 10: 2\n",
            "Sentence 11: 2\n",
            "Sentence 12: 2\n",
            "Sentence 13: 2\n",
            "Sentence 14: 2\n",
            "Sentence 15: 2\n",
            "Sentence 16: 2\n",
            "Sentence 17: 2\n",
            "Sentence 18: 2\n",
            "Sentence 19: 2\n",
            "Sentence 20: 2\n",
            "Sentence 21: 2\n",
            "Sentence 22: 2\n",
            "Sentence 23: 2\n",
            "Sentence 24: 2\n",
            "Sentence 25: 2\n",
            "Sentence 26: 2\n",
            "Sentence 27: 2\n",
            "Sentence 28: 2\n",
            "Sentence 29: 2\n",
            "Sentence 30: 2\n",
            "Sentence 31: 2\n",
            "Sentence 32: 2\n",
            "Sentence 33: 2\n",
            "Sentence 34: 2\n",
            "Sentence 35: 2\n",
            "Sentence 36: 2\n",
            "Sentence 37: 2\n",
            "Sentence 38: 2\n",
            "Sentence 39: 2\n",
            "Sentence 40: 2\n",
            "Sentence 41: 2\n",
            "Sentence 42: 2\n",
            "Sentence 43: 2\n",
            "Sentence 44: 2\n",
            "Sentence 45: 2\n",
            "Sentence 46: 2\n",
            "Sentence 47: 2\n",
            "Sentence 48: 2\n",
            "Sentence 49: 2\n",
            "Sentence 50: 2\n"
          ]
        }
      ]
    }
  ]
}